{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6rfq5oMWj5EY"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "\n",
    "#theano.config.gcc.cxxflags=\"-Wno-c++11-narrowing\"\n",
    "theano.config.gcc.cxxflags = \"-Wno-c++11-narrowing\"\n",
    "\n",
    "#ADD THIS LOGIC TO THE APPLY_DELAY FUNCTION DIRECTLY\n",
    "#def lognormal_tensor(x, mean, sigma):\n",
    "#  dist = tt.exp(-((tt.log(x)-mean) **2)/ (2*sigma**2)\n",
    "#  return dist/tt.sum(dist, axis=0)\n",
    "def make_delay_matrix(n_rows, n_columns, initial_delay=0):\n",
    "    \"\"\"\n",
    "        Has in each entry the delay between the input with size n_rows and the output\n",
    "        with size n_columns\n",
    "\n",
    "        initial_delay is the top-left element.\n",
    "    \"\"\"\n",
    "    size = max(n_rows, n_columns)\n",
    "    mat = np.zeros((size, size))\n",
    "    for i in range(size):\n",
    "        diagonal = np.ones(size - i) * (initial_delay + i)\n",
    "        mat += np.diag(diagonal, i)\n",
    "    for i in range(1, size):\n",
    "        diagonal = np.ones(size - i) * (initial_delay - i)\n",
    "        mat += np.diag(diagonal, -i)\n",
    "    return mat[:n_rows, :n_columns]\n",
    "\n",
    "def delay_cases(new_I_t, len_new_I_t, len_out, delay, delay_diff):\n",
    "    \"\"\"\n",
    "        Delays (time shifts) the input new_I_t by delay.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        new_I_t : ~numpy.ndarray or theano vector\n",
    "            Input to be delayed.\n",
    "\n",
    "        len_new_I_t : integer\n",
    "            Length of new_I_t. (Think len(new_I_t) ).\n",
    "            Assure len_new_I_t is larger then len(cum_confirmed_positive)-delay, otherwise it\n",
    "            means that the simulated data is not long enough to be fitted to the data.\n",
    "\n",
    "        len_out : integer\n",
    "            Length of the output.\n",
    "\n",
    "        delay : number\n",
    "            If delay is an integer, the array will be exactly shifted. Else, the data\n",
    "            will be shifted and intepolated (convolved with hat function of width one).\n",
    "            Take care that delay is smaller than or equal to delay_diff,\n",
    "            otherwise zeros are returned, which could potentially lead to errors.\n",
    "\n",
    "        delay_diff: integer\n",
    "            The difference in length between the new_I_t and the output.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            an array with length len_out that was time-shifted by delay\n",
    "    \"\"\"\n",
    "\n",
    "    # elementwise delay of input to output\n",
    "    delay_mat = make_delay_matrix(\n",
    "        n_rows=len_new_I_t, n_columns=len_out, initial_delay=delay_diff\n",
    "    )\n",
    "    inferred_cases = itplt(new_I_t, delay, delay_mat)\n",
    "    return inferred_cases\n",
    "\n",
    "def delay(rows, columns, delay=0): #make_Delay_matrix\n",
    "  #This looks at the shape of the parameters, and the delay, in order to create\n",
    "  #a delay matrix with numbers starting from the diagonal (the diagonal takes the value of the delay and the next values, \n",
    "  #follow an arithmetic progression with a unit increase)\n",
    "  size= max(rows, columns)\n",
    "  out = np.zeros((size, size))\n",
    "  for i in range(size):\n",
    "    del_new= np.ones(size-i)*(delay+i)\n",
    "    out = out + np.diag(del_new, i)\n",
    "  for i in range(1, size):\n",
    "    del_new = np.ones(size-i)*(delay-i)\n",
    "  return out[:rows, :columns]\n",
    "#Function lognormal_tensor added to the function delayed\n",
    "\n",
    "def itplt(arr, delay, datadelay): #Data smoothing function\n",
    "  itplt_data = tt.maximum(1 - tt.abs_(datadelay - delay), 0)\n",
    "  dotprod = tt.dot(arr, itplt_data)\n",
    "  return dotprod\n",
    "\n",
    "def delay_cases_lognormal(\n",
    "    input_arr,\n",
    "    len_input_arr,\n",
    "    len_output_arr,\n",
    "    median_delay,\n",
    "    scale_delay,\n",
    "    delay_t,\n",
    "):\n",
    "    delay_mat = delay(\n",
    "        rows=len_input_arr,\n",
    "        columns=len_output_arr,\n",
    "        delay=delay_t,\n",
    "    )\n",
    "    delay_mat[\n",
    "        delay_mat < 0.01\n",
    "    ] = 0.01  # needed because negative values lead to nans in the lognormal distribution.\n",
    "    delayed_arr = delayed(input_arr, median_delay, scale_delay, delay_mat)\n",
    "    return delayed_arr\n",
    "\n",
    "def delayed(arr, delay, delay_shape, datadelay): #apply_delay and tt_lognormal\n",
    "  distribution = tt.exp(-((tt.log(datadelay)-np.log(delay))**2)/ (2*delay_shape **2))\n",
    "  arr2 = distribution/tt.sum(distribution, axis=0)\n",
    "  return tt.dot(arr, arr2)\n",
    "\n",
    "def infer_delayed(I_rate_t,I_tperiod,  output_length, delay, diff_I_output):\n",
    "  delayed_initial = delay(rows= I_tperiod, columns = output_length, delay = diff_I_output)\n",
    "  delay_inferred = itplt(I_rate_t, delay, delayed_initial)\n",
    "  return delay_inferred\n",
    "\n",
    "def dist_smooth(v1, vk, t1, tk, t_total):\n",
    "  t = np.arange(t_total)\n",
    "  smooth = tt.clip((t - t1)/(tk- t1), 0,1) * (vk - v1) + v1 #Smoothing with delta values\n",
    "  return smooth \n",
    "#output=delay(rows= 6,columns = 4, delay = 2)\n",
    "#print(output)\n",
    "def SIR_MOD(daily_positive_cases, ordered_list_of_gov_interventions, date1, constant_parameters ):\n",
    "\n",
    "  \n",
    "  #Adding default values for when shapes and parameters are not defined in the change point list\n",
    "  #for key, value in prior_information_0.items():\n",
    "  #  if key not in prior_information:\n",
    "  #    prior_information[key] = value\n",
    "  \n",
    "\n",
    "  svi = 0.19 #CA:0.8903 #NY: 0.479 #NJ:0.66254 #BRONX COUNTY OVERALL SVI NORMALIZED: 0.2 #MARYLAND\n",
    "  with pm.Model() as sim: \n",
    "    I_start = pm.HalfNormal(name = 'start_inf', sigma = constant_parameters['prior_beta_I_start']/(1+svi))\n",
    "\n",
    "\n",
    "    list_infections = []\n",
    "\n",
    "    for i, sd_pt in enumerate(ordered_list_of_gov_interventions):\n",
    "      list_infections.append(\n",
    "          pm.Lognormal(\n",
    "              name=f'Inf_rate_{i}',\n",
    "              mu=np.log(sd_pt['prior_inf_rate_median'] ),\n",
    "              sigma = sd_pt['prior_inf_rate_sigma']\n",
    "          )\n",
    "      )\n",
    "          \n",
    "    cp_transient_list = []\n",
    "    prev_date = date1\n",
    "    for i, sd_pt in enumerate(ordered_list_of_gov_interventions[1:]):\n",
    "        \n",
    "        transient_start = sd_pt['prior_mean_transient']\n",
    "        prior_mean = (transient_start - prev_date).days\n",
    "      \n",
    "        tr_start = pm.Normal(\n",
    "                name=f\"transient_start_{i}\",\n",
    "                mu=prior_mean,\n",
    "                sigma=sd_pt[\"prior_variance_date_start_transient\"],\n",
    "            )\n",
    "        cp_transient_list.append(tr_start)\n",
    "        dt_before = transient_start\n",
    "        # same for transient times\n",
    "    tr_len_list = []\n",
    "    for i, cp in enumerate(ordered_list_of_gov_interventions[1:]):\n",
    "      tr_len = pm.Lognormal(\n",
    "          name=f\"transient_len_{i}\",\n",
    "          mu=np.log(cp[\"prior_median_transient_len\"]),\n",
    "          sigma=cp[\"prior_variance_transient_len\"],)\n",
    "      tr_len_list.append(tr_len)\n",
    "\n",
    "    Inf_rate_t_list = [list_infections[0] * tt.ones(constant_parameters['num_days_simulation'])]\n",
    "    Inf_rate_before = list_infections[0]\n",
    "\n",
    "    for tr_start, tr_len, Inf_rate_after in zip(\n",
    "            cp_transient_list, tr_len_list, list_infections[1:]\n",
    "        ):\n",
    "        Inf_rate_t = dist_smooth(\n",
    "                          v1=0,\n",
    "                vk=1,\n",
    "                t1=tr_start,\n",
    "                tk=tr_start + tr_len,\n",
    "                t_total=constant_parameters['num_days_simulation'],\n",
    "            ) * (Inf_rate_after - Inf_rate_before)\n",
    "        Inf_rate_before = Inf_rate_after\n",
    "        Inf_rate_t_list.append(Inf_rate_t)\n",
    "    Inf_rate_t = sum(Inf_rate_t_list)\n",
    "\n",
    "        # fraction of people that recover each day, recovery rate mu\n",
    "    mu = pm.Lognormal(\n",
    "            name=\"mu\",\n",
    "            mu=np.log(constant_parameters[\"prior_median_mu\"]),\n",
    "            sigma=constant_parameters[\"prior_variance_mu\"],\n",
    "        )\n",
    "\n",
    "        # delay in days between contracting the disease and being recorded\n",
    "    delay = pm.Lognormal(\n",
    "            name=\"delay\",\n",
    "            mu=np.log(constant_parameters[\"prior_median_delay\"]),\n",
    "            sigma=constant_parameters[\"prior_variance_delay\"],\n",
    "        )\n",
    "\n",
    "        # prior of the error of observed cases\n",
    "    sigma_obs = pm.HalfNormal(\"sigma_obs\", sigma=constant_parameters[\"prior_beta_variance_obs\"])\n",
    "\n",
    "        # -------------------------------------------------------------------------- #\n",
    "        # training the model with loaded data provided as argument\n",
    "        # -------------------------------------------------------------------------- #\n",
    "\n",
    "    S_start = constant_parameters['tot_pop'] - I_start\n",
    "\n",
    "    new_I_0 = tt.zeros_like(I_start)\n",
    "    S, I, new_I = Model_simulation(\n",
    "            Inf_rate_t=Inf_rate_t, mu=mu, S_start=S_start, I_start=I_start, N=constant_parameters['tot_pop']\n",
    "        )\n",
    "    '''\n",
    "    def subsequent_day_parameters(Inf_rate_t, S_t, I_t, _,mu, tot_pop):\n",
    "      new_I_t = Inf_rate_t / tot_pop * I_t * S_t\n",
    "      S_t = S_t - new_I_t\n",
    "      I_t = I_t + new_I_t - mu * I_t\n",
    "      I_t = tt.clip(I_t, 0, tot_pop) #for stability\n",
    "      return S_t, I_t, new_I_t\n",
    "\n",
    "      # first tuple of theano scan will return S, I, new_I\n",
    "    outputs, _ = theano.scan(\n",
    "        fn=subsequent_day_parameters,\n",
    "        sequences=[Inf_rate_t],\n",
    "        outputs_info=[S_start, I_start, new_I_0],\n",
    "        non_sequences=[mu, constant_parameters['tot_pop']],)\n",
    "\n",
    "    S, I, new_I = outputs\n",
    "    '''\n",
    "\n",
    "    new_cases_inferred = delay_cases(\n",
    "        new_I_t=new_I,\n",
    "        len_new_I_t=constant_parameters['num_days_simulation'],\n",
    "        len_out=constant_parameters['num_days_simulation'] - constant_parameters['diff_data_simulation'],\n",
    "        delay=delay,\n",
    "        delay_diff=constant_parameters['diff_data_simulation'],)\n",
    "    \n",
    "    \n",
    "    new_cases_inferred_eff = new_cases_inferred\n",
    "    num_days_data = daily_positive_cases.shape[-1]\n",
    "    \n",
    "    pm.StudentT(\n",
    "        name=\"_new_cases_studentT\",\n",
    "        nu=4,\n",
    "        mu=new_cases_inferred_eff[:num_days_data],\n",
    "        sigma=tt.abs_(new_cases_inferred[:num_days_data] + 1) ** 0.5\n",
    "        * sigma_obs,  #+1 and tt.abs to avoid nans\n",
    "        observed=daily_positive_cases,)\n",
    "    \n",
    "    pm.Deterministic(\"Inf_rate_t\", Inf_rate_t)\n",
    "    pm.Deterministic(\"new_cases\", new_cases_inferred_eff)\n",
    "    pm.Deterministic(\"new_cases_raw\", new_cases_inferred)\n",
    "\n",
    "  return sim\n",
    "\n",
    "def Model_simulation(Inf_rate_t, mu, S_start, I_start, N):\n",
    "    \"\"\"\n",
    "        Implements the susceptible-infected-recovered model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Inf_rate_t : ~numpy.ndarray\n",
    "            time series of spreading rate, the length of the array sets the\n",
    "            number of steps to run the model for\n",
    "\n",
    "        mu : number\n",
    "            recovery rate\n",
    "\n",
    "        S_start : number\n",
    "            initial number of susceptible at first time step\n",
    "\n",
    "        I_start : number\n",
    "            initial number of infected\n",
    "\n",
    "        N : number\n",
    "            population size\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        S : array\n",
    "            time series of the susceptible\n",
    "\n",
    "        I : array\n",
    "            time series of the infected\n",
    "\n",
    "        new_I : array\n",
    "            time series of the new infected\n",
    "    \"\"\"\n",
    "\n",
    "    new_I_0 = tt.zeros_like(I_start)\n",
    "\n",
    "    def next_day(Inf_rate_t, S_t, I_t, _, mu, N):\n",
    "        new_I_t = Inf_rate_t / N * I_t * S_t\n",
    "        S_t = S_t - new_I_t\n",
    "        I_t = I_t + new_I_t - mu * I_t\n",
    "        I_t = tt.clip(I_t, 0, N)  # for stability\n",
    "        return S_t, I_t, new_I_t\n",
    "\n",
    "    # theano scan returns two tuples, first one containing a time series of\n",
    "    # what we give in outputs_info : S, I, new_I\n",
    "    outputs, _ = theano.scan(\n",
    "        fn=next_day,\n",
    "        sequences=[Inf_rate_t],\n",
    "        outputs_info=[S_start, I_start, new_I_0],\n",
    "        non_sequences=[mu, N],\n",
    "    )\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "NsTyL1tGy50h",
    "outputId": "3a3a56d1-c0f6-46c9-ca56-d2af6aba5f86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/amanchawla'"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "DkT811pBbN3r",
    "outputId": "ef5882e4-3d9a-4b91-fa73-f2a46ce84813"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndata=pd.read_csv('cases_by_county_0804.csv')\\ndata\\nIndex=data.columns\\nNYCounty=data[data['County Name']=='Bronx County']\\nNYCounty=NYCounty.T\\nNYCounty2=NYCounty[4:]\\nNYCounty\\nNYCounty2['State']='NY'\\nNYCounty2['Dates']=NYCounty2.index\\nNYCounty2\\ndata=NYCounty2\\ndata.columns=['Bronx County', 'State', 'Dates']\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "data=pd.read_csv('cases_by_county_0804.csv')\n",
    "data\n",
    "Index=data.columns\n",
    "NYCounty=data[data['County Name']=='Bronx County']\n",
    "NYCounty=NYCounty.T\n",
    "NYCounty2=NYCounty[4:]\n",
    "NYCounty\n",
    "NYCounty2['State']='NY'\n",
    "NYCounty2['Dates']=NYCounty2.index\n",
    "NYCounty2\n",
    "data=NYCounty2\n",
    "data.columns=['Bronx County', 'State', 'Dates']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "colab_type": "code",
    "id": "gWwJCtitSDuy",
    "outputId": "517d48ad-60aa-4599-c65a-001932a1cf67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Dates  AK  AL  AR  AZ  CA  CO  CT  DC  DE  FL  GA  HI  IA  ID  IL  IN  \\\n",
      "0  1/22/20   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "1  1/23/20   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "2  1/24/20   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   \n",
      "3  1/25/20   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   \n",
      "4  1/26/20   0   0   0   1   2   0   0   0   0   0   0   0   0   0   1   0   \n",
      "\n",
      "   KS  KY  LA  MA  MD  ME  MI  MN  MO  MS  MT  NC  ND  NE  NH  NJ  NM  NV  NY  \\\n",
      "0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "3   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "4   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "\n",
      "   OH  OK  OR  PA  RI  SC  SD  TN  TX  UT  VA  VT  WA  WI  WV  WY  \n",
      "0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0  \n",
      "1   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0  \n",
      "2   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0  \n",
      "3   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0  \n",
      "4   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using advi...\n",
      "Average Loss = 236.26:  11%|█▏        | 22996/200000 [03:43<28:42, 102.78it/s]\n",
      "Convergence achieved at 23000\n",
      "Interrupted at 22,999 [11%]: Average Loss = 317.77\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [sigma_obs, delay, mu, transient_len_1, transient_len_0, transient_start_1, transient_start_0, Inf_rate_2, Inf_rate_1, Inf_rate_0, start_inf]\n",
      "Sampling 2 chains, 0 divergences: 100%|██████████| 10000/10000 [2:03:04<00:00,  1.35draws/s]\n",
      "The number of effective samples is smaller than 25% for some parameters.\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('Confirmed_cases_by_state.csv') #FOR STATE DATA\n",
    "\n",
    "\n",
    "population_lockdown_dates=pd.read_csv('LockdownDates_populationMerged.csv')\n",
    "country = 'United States'\n",
    "state_codes=data.columns[1:]\n",
    "\n",
    "date_data_begin = max(pd.to_datetime(data['Dates'], infer_datetime_format=True)).to_pydatetime() - datetime.timedelta(days=40)\n",
    "date_data_end   = max(pd.to_datetime(data['Dates'], infer_datetime_format=True)).to_pydatetime()\n",
    "data2=data[pd.to_datetime(data['Dates'], infer_datetime_format=True)>=date_data_begin]\n",
    "\n",
    "\n",
    "population_lockdown_dates.loc[population_lockdown_dates['lockdown_date']=='YTA','lockdown_date']=date_data_end\n",
    "population_lockdown_dates.loc[population_lockdown_dates['partial_date']=='YTA','lockdown_date']=date_data_end\n",
    "population_lockdown_dates.loc[population_lockdown_dates['partial_date']=='YTA','partial_date']=date_data_end\n",
    "\n",
    "population_lockdown_dates['partial_date']=population_lockdown_dates['partial_date'].fillna(date_data_end)\n",
    "population_lockdown_dates['lockdown_date']=population_lockdown_dates['lockdown_date'].fillna(date_data_end)\n",
    "population_lockdown_dates['lockdown_date']=pd.to_datetime(population_lockdown_dates['lockdown_date'], infer_datetime_format=True)\n",
    "population_lockdown_dates.loc[population_lockdown_dates['lockdown_date']<date_data_begin,'lockdown_date']=date_data_end\n",
    "population_lockdown_dates['partial_date']=pd.to_datetime(population_lockdown_dates['partial_date'], infer_datetime_format=True)\n",
    "population_lockdown_dates.loc[population_lockdown_dates['lockdown_date']<population_lockdown_dates['partial_date'],'partial_date']=population_lockdown_dates['lockdown_date']\n",
    "population_lockdown_dates.dtypes\n",
    "state_codes=data.columns[1:]\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(data.head())\n",
    "\n",
    "\n",
    "#FOR STATE DATA (i should correspond to the state code we're running it for in state_code)\n",
    "i=20 #4: California; 31: NEw Jersey, 34: New York, 20: Maryland\n",
    "state=state_codes[i]\n",
    "cases_obs = data2[state].values\n",
    "population_lockdown_dates_sub=population_lockdown_dates.loc[population_lockdown_dates['State Code']==state]\n",
    "population_lockdown_dates_sub\n",
    "\n",
    "#prior_date_mild_dist_begin = datetime.datetime.utcfromtimestamp(population_lockdown_dates_sub['partial_date'].values[0].tolist()/1e9)- datetime.timedelta(days = 3) \n",
    "#prior_date_mild_dist_begin\n",
    "\n",
    "diff_data_sim = 14\n",
    "date_begin_sim = date_data_begin - datetime.timedelta(days = diff_data_sim) \n",
    "\n",
    "prior_date_mild_dist_begin = datetime.datetime.utcfromtimestamp(population_lockdown_dates_sub['partial_date'].values[0].tolist()/1e9)- datetime.timedelta(days = 3) \n",
    "prior_date_strong_dist_begin =  datetime.datetime.utcfromtimestamp(population_lockdown_dates_sub['partial_date'].values[0].tolist()/1e9)- datetime.timedelta(days = 2) \n",
    "prior_date_contact_ban_begin =  datetime.datetime.utcfromtimestamp(population_lockdown_dates_sub['lockdown_date'].values[0].tolist()/1e9)- datetime.timedelta(days = 1) \n",
    "\n",
    "num_days_data = (date_data_end-date_data_begin).days\n",
    "num_days_future = 110\n",
    "date_begin_sim = date_data_begin - datetime.timedelta(days = diff_data_sim)\n",
    "date_end_sim   = date_data_end   + datetime.timedelta(days = num_days_future)\n",
    "num_days_sim = (date_end_sim-date_begin_sim).days\n",
    "\n",
    "\n",
    "\n",
    "#what if scenarios\n",
    "#intervention attribute dictionary\n",
    "\n",
    "\n",
    "###################################################################\n",
    "#dictionary format for adding new goverment interventions##########\n",
    "###################################################################\n",
    "\n",
    "\n",
    "intervetion_attributes = dict(\n",
    "    #date of intervention begin\n",
    "    prior_mean_transient=None,\n",
    "    prior_variance_date_start_transient=None,\n",
    "    \n",
    "    #time for intervebtion to take effect\n",
    "    prior_median_transient_len=None,\n",
    "    prior_variance_transient_len=None,\n",
    "    \n",
    "    #infection rate\n",
    "    prior_inf_rate_median= None,\n",
    "    prior_inf_rate_sigma= None,\n",
    "    \n",
    ")\n",
    "\n",
    "###################################################################\n",
    "#define new goverment interventions################################\n",
    "###################################################################\n",
    "\n",
    "#defining default prior\n",
    "default_prior = {\n",
    "    #date of intervention begin\n",
    "    'prior_mean_transient':None,\n",
    "    'prior_variance_date_start_transient':None,                \n",
    "    #time for intervebtion to take effect\n",
    "    'prior_median_transient_len':None,\n",
    "    'prior_variance_transient_len':None,\n",
    "    #infection rate\n",
    "    'prior_inf_rate_median': 0.4,\n",
    "    'prior_inf_rate_sigma': 0.9}\n",
    "                    \n",
    "#defining changept 2 -  mild_social_distancing\n",
    "mild_social_distancing = {\n",
    "                    #date of intervention begin\n",
    "                    'prior_mean_transient':prior_date_mild_dist_begin,\n",
    "                    'prior_variance_date_start_transient':3,\n",
    "                    \n",
    "                    #time for intervebtion to take effect\n",
    "                    'prior_median_transient_len':3,\n",
    "                    'prior_variance_transient_len':0.3,\n",
    "                    \n",
    "                    #infection rate\n",
    "                    'prior_inf_rate_median': 0.2,\n",
    "                    'prior_inf_rate_sigma': 0.5\n",
    "                    \n",
    "                }\n",
    "\n",
    "#defining changept 3 -  strong social distancing\n",
    "strong_social_distancing = {\n",
    "                    #date of intervention begin\n",
    "                    'prior_mean_transient':prior_date_strong_dist_begin,\n",
    "                    'prior_variance_date_start_transient':1,\n",
    "                    \n",
    "                    #time for intervebtion to take effect\n",
    "                    'prior_median_transient_len':3,\n",
    "                    'prior_variance_transient_len':0.3,\n",
    "                    \n",
    "                    #infection rate\n",
    "                    'prior_inf_rate_median': 1/8,\n",
    "                    'prior_inf_rate_sigma': 0.5\n",
    "                    \n",
    "                }\n",
    "\n",
    "\n",
    "#defining changept 3 -  strong social distancing\n",
    "lock_down = {\n",
    "                    #date of intervention begin\n",
    "                    'prior_mean_transient':prior_date_contact_ban_begin,\n",
    "                    'prior_variance_date_start_transient':1,\n",
    "                    \n",
    "                    #time for intervebtion to take effect\n",
    "                    'prior_median_transient_len':3,\n",
    "                    'prior_variance_transient_len':0.3,\n",
    "                    \n",
    "                    #infection rate\n",
    "                    'prior_inf_rate_median': 1/8/2,\n",
    "                    'prior_inf_rate_sigma': 0.5\n",
    "                    \n",
    "                }\n",
    "\n",
    "\n",
    "ordered_list_of_gov_interventions = [ default_prior, mild_social_distancing, strong_social_distancing, lock_down]\n",
    "\n",
    "\n",
    "constant_parameters = {\n",
    "          'tot_pop': population_lockdown_dates_sub['Population'].values[0],\n",
    "          'prior_beta_I_start' : 100,\n",
    "          'prior_median_mu' : 0.12, \n",
    "          'prior_variance_mu' : 0.2,\n",
    "          'prior_median_delay' : 5, \n",
    "          'prior_variance_delay' : 0.2,\n",
    "          'prior_beta_variance_obs' : 10,\n",
    "          #simulation inofrmation\n",
    "          'num_days_simulation' : num_days_sim,\n",
    "          'diff_data_simulation' : diff_data_sim\n",
    "          }\n",
    "\n",
    "traces = []\n",
    "models=[]\n",
    "for scenarios in np.arange(1,4):\n",
    "  model = SIR_MOD(daily_positive_cases= np.diff(cases_obs), \n",
    "                ordered_list_of_gov_interventions = ordered_list_of_gov_interventions[:scenarios], \n",
    "                date1 = date_begin_sim, \n",
    "                constant_parameters = constant_parameters)\n",
    "  models.append(model)\n",
    "  traces.append(pm.sample(model=model, init='advi', draws=4000, tune=1000 ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kJC3DH9omreF"
   },
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "qeZHjYgIJZvi",
    "outputId": "b63bdf52-1ac4-48f7-9ddc-1ef812ffb782"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n"
     ]
    }
   ],
   "source": [
    "def truncate_number(number, precision):\n",
    "    return '{{:.{}f}}'.format(precision).format(number)  \n",
    "\n",
    "def print_median_CI(arr, prec = 2):\n",
    "    f_trunc = lambda n: truncate_number(n, prec)\n",
    "    med = f_trunc(np.median(arr))\n",
    "    perc1, perc2 = f_trunc(np.percentile(arr, q=2.5)), f_trunc(np.percentile(arr, q=97.5))\n",
    "    return 'Median: {}\\n95% CI: [{}, {}]'.format(med, perc1, perc2)\n",
    "def conv_time_to_mpl_dates(arr):\n",
    "    return matplotlib.dates.date2num([datetime.timedelta(days=float(date)) + date_begin_sim for date in arr])\n",
    "traces_copy=traces\n",
    "trace = traces[0]\n",
    "posterior = traces\n",
    "\n",
    "colors  = ['tab:green']\n",
    "#points = [ 'Mild Social Distancing', 'Strong Social Distancing','Total Lockdown']\n",
    "points = [ 'Mild Social Distancing','Strong Social Distancing','Total Lockdown']\n",
    "\n",
    "new_cases_past=pd.DataFrame()\n",
    "new_cases_Future=pd.DataFrame()\n",
    "new_cases_Future_percentiles=pd.DataFrame()\n",
    "new_cases_Future_percentiles_DF=pd.DataFrame()\n",
    "for trace_scen,  point in zip(posterior,  points):\n",
    "    new_cases_past1 = trace_scen.new_cases[:,:num_days_data]\n",
    "    new_cases_past[point]=np.median(new_cases_past1, axis=0)\n",
    "    time2 = np.arange(0, num_days_future+1)\n",
    "    mpl_dates_fut = conv_time_to_mpl_dates(time2) + diff_data_sim + num_days_data\n",
    "    end_date = mpl_dates_fut[-10]\n",
    "    cases_future = trace_scen['new_cases'][:, num_days_data:].T\n",
    "    new_cases_Future[point] = np.median(cases_future, axis=-1)\n",
    "    new_cases_Future_percentiles[point] = (\n",
    "    np.percentile(cases_future, q=2.5, axis=-1),\n",
    "    np.percentile(cases_future, q=97.5, axis=-1),\n",
    "    )\n",
    "   \n",
    "    new_cases_Future_percentiles_DF[f'{point} - Lower Bound'] = new_cases_Future_percentiles[point][0]\n",
    "    new_cases_Future_percentiles_DF[f'{point} - Upper Bound'] = new_cases_Future_percentiles[point][1]\n",
    "    \n",
    "Date_sim=[]\n",
    "for i in range(1, len(time2)):\n",
    "  Date_sim.append(date_begin_sim +datetime.timedelta(days=float(time2[i]))+datetime.timedelta(days=float(54)))\n",
    "date_sim2=[]\n",
    "for date in Date_sim:\n",
    "  date_sim2.append(datetime.datetime.strftime(date, '%d/%m/%y'))\n",
    "print(len(date_sim2[1:]))\n",
    "date_sim3=date_sim2[1:]\n",
    "new_cases_Future\n",
    "new_cases_Future_cum=new_cases_Future.cumsum(axis=0)\n",
    "new_cases_Future_cum1=new_cases_Future_cum+cases_obs[-1]\n",
    "new_cases_Future_cum1\n",
    "\n",
    "new_cases_Future_cum2=new_cases_Future_cum1\n",
    "new_cases_Future_cum2.index=date_sim2\n",
    "new_cases_Future_cum2 #SAVE THIS \n",
    "CUM_Cases_Future_percentiles=pd.DataFrame()\n",
    "Cumulative_Cases=pd.DataFrame()\n",
    "cum_cases_Future_percentiles_DF=pd.DataFrame()\n",
    "for trace_scen,  point in zip(posterior, points):\n",
    "    new_cases_past = trace_scen.new_cases[:,:num_days_data]\n",
    "    cum_cases = np.cumsum(new_cases_past, axis=1) + cases_obs[0]\n",
    "    Cumulative_Cases[point]=np.median(cum_cases, axis=0)\n",
    "    time2 = np.arange(0, num_days_future+1)\n",
    "    mpl_dates_fut = conv_time_to_mpl_dates(time2) + diff_data_sim + num_days_data\n",
    "    cases_future = np.cumsum(trace_scen['new_cases'][:, num_days_data:].T, axis=0) + cases_obs[-1]\n",
    "    \n",
    "    #cases_future = np.concatenate([np.ones((1,cases_future.shape[1]))*cases_obs[-1], cases_future], axis=0)\n",
    "    #Cumulative_Cases[legend] = np.median(cases_future, axis=-1)\n",
    "    CUM_Cases_Future_percentiles[point] = (\n",
    "        np.percentile(cases_future, q=2.5, axis=-1),\n",
    "        np.percentile(cases_future, q=97.5, axis=-1),)\n",
    "    cum_cases_Future_percentiles_DF[f'{point}- Lower Bound'] = CUM_Cases_Future_percentiles[point][0]\n",
    "    cum_cases_Future_percentiles_DF[f'{point} - Upper Bound'] = CUM_Cases_Future_percentiles[point][1]\n",
    "\n",
    "\n",
    "cum_cases_Future_percentiles_DF.index=date_sim2\n",
    "cum_cases_Future_percentiles_DF\n",
    "cum_cases_Future_percentiles_DF['Date']=cum_cases_Future_percentiles_DF.index.astype('str')\n",
    "new_cases_Future_cum2['Date']=new_cases_Future_cum2.index.astype('str')\n",
    "FINAL_DF=pd.merge(cum_cases_Future_percentiles_DF, new_cases_Future_cum2, on=\"Date\")\n",
    "\n",
    "\n",
    "FINAL_DF.index=FINAL_DF['Date']\n",
    "FINAL_DF2=FINAL_DF.copy()\n",
    "\n",
    "#FINAL_DF2.to_csv('California_0804_Projection_Data2_1.csv')\n",
    "#FINAL_DF2.to_csv('California_2604_Projection_Data2_1.csv')\n",
    "#FINAL_DF2.to_csv('N_1604_Projection_Data2_1.csv')\n",
    "FINAL_DF2.head()\n",
    "FINAL_DF2.to_csv('MarylandState_new_code_projection_08_04_Lockdown.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IKt1KjhrdVS0"
   },
   "outputs": [],
   "source": [
    " FINAL_DF2.to_csv('MarylandStste_new_code_projection_08_04_Lockdown.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8PENIYzZR5nP"
   },
   "outputs": [],
   "source": [
    "def delay(rows, columns, delay=0): #make_Delay_matrix\n",
    "  #This looks at the shape of the parameters, and the delay, in order to create\n",
    "  #a delay matrix with numbers starting from the diagonal (the diagonal takes the value of the delay and the next values, \n",
    "  #follow an arithmetic progression with a unit increase)\n",
    "  size= max(rows, columns)\n",
    "  out = np.zeros((size, size))\n",
    "  for i in range(size):\n",
    "    del_new= np.ones(size-i)*(delay+i)\n",
    "    out = out + np.diag(del_new, i)\n",
    "  for i in range(1, size):\n",
    "    del_new = np.ones(size-i)*(delay-i)\n",
    "  return out[:rows, :columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "colab_type": "code",
    "id": "sHAgWPFvR6_9",
    "outputId": "d3496f50-9c0b-4a97-bde4-7a8b93f45192"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14., 15., 16., ..., 61., 62., 63.],\n",
       "       [ 0., 14., 15., ..., 60., 61., 62.],\n",
       "       [ 0.,  0., 14., ..., 59., 60., 61.],\n",
       "       ...,\n",
       "       [ 0.,  0.,  0., ..., 14., 15., 16.],\n",
       "       [ 0.,  0.,  0., ...,  0., 14., 15.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0., 14.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delay(50,50,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "q1VLz3_aTHLL",
    "outputId": "78d6006e-3a4b-4c02-d9a6-be0e6e292b27"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>lockdown_date</th>\n",
       "      <th>partial_date</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>2020-04-04</td>\n",
       "      <td>2020-04-04</td>\n",
       "      <td>AL</td>\n",
       "      <td>4903185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>2020-03-18</td>\n",
       "      <td>2020-03-18</td>\n",
       "      <td>AK</td>\n",
       "      <td>731545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>AZ</td>\n",
       "      <td>7278717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>AR</td>\n",
       "      <td>3017804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>California</td>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>CA</td>\n",
       "      <td>39512223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>CO</td>\n",
       "      <td>5758736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Connecticut</td>\n",
       "      <td>2020-03-23</td>\n",
       "      <td>2020-03-23</td>\n",
       "      <td>CT</td>\n",
       "      <td>3565287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Delaware</td>\n",
       "      <td>2020-03-24</td>\n",
       "      <td>2020-03-24</td>\n",
       "      <td>DE</td>\n",
       "      <td>973764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Florida</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>FL</td>\n",
       "      <td>21477737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>GA</td>\n",
       "      <td>10617423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>2020-03-17</td>\n",
       "      <td>2020-03-17</td>\n",
       "      <td>HI</td>\n",
       "      <td>1415872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Idaho</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>ID</td>\n",
       "      <td>1787065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>2020-03-21</td>\n",
       "      <td>2020-03-21</td>\n",
       "      <td>IL</td>\n",
       "      <td>12671821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>2020-03-24</td>\n",
       "      <td>2020-03-24</td>\n",
       "      <td>IN</td>\n",
       "      <td>6732219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Iowa</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>IA</td>\n",
       "      <td>3155070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>KS</td>\n",
       "      <td>2913314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>KY</td>\n",
       "      <td>4467673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>LA</td>\n",
       "      <td>4648794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Maine</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>ME</td>\n",
       "      <td>1344212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>MD</td>\n",
       "      <td>6045680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>2020-03-24</td>\n",
       "      <td>MA</td>\n",
       "      <td>6892503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>2020-03-24</td>\n",
       "      <td>2020-03-24</td>\n",
       "      <td>MI</td>\n",
       "      <td>9986857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>MN</td>\n",
       "      <td>5639632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>Mississippi</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>MS</td>\n",
       "      <td>2976149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>MO</td>\n",
       "      <td>6137428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>Montana</td>\n",
       "      <td>2020-03-28</td>\n",
       "      <td>2020-03-28</td>\n",
       "      <td>MT</td>\n",
       "      <td>1068778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>Nebraska</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>NE</td>\n",
       "      <td>1934408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>Nevada</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>2020-03-17</td>\n",
       "      <td>NV</td>\n",
       "      <td>3080156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>NH</td>\n",
       "      <td>1359711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>2020-03-21</td>\n",
       "      <td>NJ</td>\n",
       "      <td>8882190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>New Mexico</td>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>2020-03-15</td>\n",
       "      <td>NM</td>\n",
       "      <td>2096829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>New York</td>\n",
       "      <td>2020-03-21</td>\n",
       "      <td>2020-03-21</td>\n",
       "      <td>NY</td>\n",
       "      <td>19453561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>NC</td>\n",
       "      <td>10488084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>North Dakota</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>ND</td>\n",
       "      <td>762062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>OH</td>\n",
       "      <td>11689100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>OK</td>\n",
       "      <td>3956971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>OR</td>\n",
       "      <td>4217737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>PA</td>\n",
       "      <td>12801989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>Rhode Island</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>2020-03-23</td>\n",
       "      <td>RI</td>\n",
       "      <td>1059361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>2020-06-04</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>SC</td>\n",
       "      <td>5148714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>South Dakota</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>SD</td>\n",
       "      <td>884659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>TN</td>\n",
       "      <td>6829174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>Texas</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>TX</td>\n",
       "      <td>28995881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>Utah</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>UT</td>\n",
       "      <td>3205958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>Vermont</td>\n",
       "      <td>2020-03-24</td>\n",
       "      <td>2020-03-24</td>\n",
       "      <td>VT</td>\n",
       "      <td>623989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>VIRGINIA</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>VA</td>\n",
       "      <td>8535519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>WASHINGTON, D.C.</td>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>WA</td>\n",
       "      <td>7614893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>West Virginia</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>WV</td>\n",
       "      <td>1792147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>WI</td>\n",
       "      <td>5822434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>WY</td>\n",
       "      <td>578759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               state lockdown_date partial_date State Code  Population\n",
       "0            ALABAMA    2020-04-04   2020-04-04         AL     4903185\n",
       "1             Alaska    2020-03-18   2020-03-18         AK      731545\n",
       "2            Arizona    2020-03-30   2020-03-30         AZ     7278717\n",
       "3           Arkansas    2020-03-16   2020-03-16         AR     3017804\n",
       "4         California    2020-03-19   2020-03-19         CA    39512223\n",
       "5           Colorado    2020-03-26   2020-03-26         CO     5758736\n",
       "6        Connecticut    2020-03-23   2020-03-23         CT     3565287\n",
       "7           Delaware    2020-03-24   2020-03-24         DE      973764\n",
       "8            Florida    2020-04-07   2020-04-07         FL    21477737\n",
       "9            Georgia    2020-04-07   2020-04-07         GA    10617423\n",
       "10            Hawaii    2020-03-17   2020-03-17         HI     1415872\n",
       "11             Idaho    2020-03-25   2020-03-25         ID     1787065\n",
       "12          Illinois    2020-03-21   2020-03-21         IL    12671821\n",
       "13           Indiana    2020-03-24   2020-03-24         IN     6732219\n",
       "14              Iowa    2020-04-07   2020-04-07         IA     3155070\n",
       "15            Kansas    2020-03-30   2020-03-30         KS     2913314\n",
       "16          Kentucky    2020-03-16   2020-03-16         KY     4467673\n",
       "17         Louisiana    2020-03-22   2020-03-22         LA     4648794\n",
       "18             Maine    2020-03-25   2020-03-25         ME     1344212\n",
       "19          Maryland    2020-03-30   2020-03-30         MD     6045680\n",
       "20     Massachusetts    2020-04-07   2020-03-24         MA     6892503\n",
       "21          Michigan    2020-03-24   2020-03-24         MI     9986857\n",
       "22         Minnesota    2020-03-27   2020-03-27         MN     5639632\n",
       "23       Mississippi    2020-04-07   2020-04-07         MS     2976149\n",
       "24          Missouri    2020-04-07   2020-04-07         MO     6137428\n",
       "25           Montana    2020-03-28   2020-03-28         MT     1068778\n",
       "26          Nebraska    2020-04-07   2020-03-16         NE     1934408\n",
       "27            Nevada    2020-04-07   2020-03-17         NV     3080156\n",
       "28     New Hampshire    2020-03-26   2020-03-26         NH     1359711\n",
       "29        New Jersey    2020-04-07   2020-03-21         NJ     8882190\n",
       "30        New Mexico    2020-03-22   2020-03-15         NM     2096829\n",
       "31          New York    2020-03-21   2020-03-21         NY    19453561\n",
       "32    NORTH CAROLINA    2020-03-30   2020-03-30         NC    10488084\n",
       "33      North Dakota    2020-04-07   2020-04-07         ND      762062\n",
       "34              Ohio    2020-03-22   2020-03-22         OH    11689100\n",
       "35          Oklahoma    2020-04-07   2020-03-26         OK     3956971\n",
       "36            Oregon    2020-04-07   2020-03-16         OR     4217737\n",
       "37      Pennsylvania    2020-04-07   2020-03-19         PA    12801989\n",
       "38      Rhode Island    2020-04-07   2020-03-23         RI     1059361\n",
       "39    South Carolina    2020-06-04   2020-04-07         SC     5148714\n",
       "40      South Dakota    2020-04-07   2020-04-07         SD      884659\n",
       "41         Tennessee    2020-04-07   2020-04-07         TN     6829174\n",
       "42             Texas    2020-04-07   2020-04-07         TX    28995881\n",
       "43              Utah    2020-04-07   2020-04-07         UT     3205958\n",
       "44           Vermont    2020-03-24   2020-03-24         VT      623989\n",
       "45          VIRGINIA    2020-03-30   2020-03-30         VA     8535519\n",
       "46  WASHINGTON, D.C.    2020-03-22   2020-03-22         WA     7614893\n",
       "47     West Virginia    2020-03-25   2020-03-25         WV     1792147\n",
       "48         Wisconsin    2020-03-25   2020-03-25         WI     5822434\n",
       "49           Wyoming    2020-04-07   2020-04-07         WY      578759"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "population_lockdown_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "6Z64t6ZlTXv_",
    "outputId": "f4a4751d-4c29-4013-da21-91348ffc95e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(1, 4)\n"
     ]
    }
   ],
   "source": [
    "print(range(1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "pzDHerQgUAvh",
    "outputId": "865d348b-2d53-4314-818d-7bb415451445"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "4U-3tPZ-UHXK",
    "outputId": "8153a28b-7668-4a13-c636-96a0a3cc1655"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(1, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5QuIQHKkGNjh"
   },
   "source": [
    "Authors: Sachin C S (Cognizant), Aman Chawla (Cognizant)\n",
    "\n",
    "Copyright {2020} Cognizant Technology Solutions\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NEW_CODE_RUN_v0_5_SVI_TOTAL_Maryland.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
