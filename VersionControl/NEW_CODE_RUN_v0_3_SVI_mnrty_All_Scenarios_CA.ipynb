{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6rfq5oMWj5EY"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "\n",
    "#theano.config.gcc.cxxflags=\"-Wno-c++11-narrowing\"\n",
    "theano.config.gcc.cxxflags = \"-Wno-c++11-narrowing\"\n",
    "\n",
    "#ADD THIS LOGIC TO THE APPLY_DELAY FUNCTION DIRECTLY\n",
    "#def lognormal_tensor(x, mean, sigma):\n",
    "#  dist = tt.exp(-((tt.log(x)-mean) **2)/ (2*sigma**2)\n",
    "#  return dist/tt.sum(dist, axis=0)\n",
    "def make_delay_matrix(n_rows, n_columns, initial_delay=0):\n",
    "    \"\"\"\n",
    "        Has in each entry the delay between the input with size n_rows and the output\n",
    "        with size n_columns\n",
    "\n",
    "        initial_delay is the top-left element.\n",
    "    \"\"\"\n",
    "    size = max(n_rows, n_columns)\n",
    "    mat = np.zeros((size, size))\n",
    "    for i in range(size):\n",
    "        diagonal = np.ones(size - i) * (initial_delay + i)\n",
    "        mat += np.diag(diagonal, i)\n",
    "    for i in range(1, size):\n",
    "        diagonal = np.ones(size - i) * (initial_delay - i)\n",
    "        mat += np.diag(diagonal, -i)\n",
    "    return mat[:n_rows, :n_columns]\n",
    "\n",
    "def delay_cases(new_I_t, len_new_I_t, len_out, delay, delay_diff):\n",
    "    \"\"\"\n",
    "        Delays (time shifts) the input new_I_t by delay.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        new_I_t : ~numpy.ndarray or theano vector\n",
    "            Input to be delayed.\n",
    "\n",
    "        len_new_I_t : integer\n",
    "            Length of new_I_t. (Think len(new_I_t) ).\n",
    "            Assure len_new_I_t is larger then len(cum_confirmed_positive)-delay, otherwise it\n",
    "            means that the simulated data is not long enough to be fitted to the data.\n",
    "\n",
    "        len_out : integer\n",
    "            Length of the output.\n",
    "\n",
    "        delay : number\n",
    "            If delay is an integer, the array will be exactly shifted. Else, the data\n",
    "            will be shifted and intepolated (convolved with hat function of width one).\n",
    "            Take care that delay is smaller than or equal to delay_diff,\n",
    "            otherwise zeros are returned, which could potentially lead to errors.\n",
    "\n",
    "        delay_diff: integer\n",
    "            The difference in length between the new_I_t and the output.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            an array with length len_out that was time-shifted by delay\n",
    "    \"\"\"\n",
    "\n",
    "    # elementwise delay of input to output\n",
    "    delay_mat = make_delay_matrix(\n",
    "        n_rows=len_new_I_t, n_columns=len_out, initial_delay=delay_diff\n",
    "    )\n",
    "    inferred_cases = itplt(new_I_t, delay, delay_mat)\n",
    "    return inferred_cases\n",
    "\n",
    "def delay(rows, columns, delay=0): #make_Delay_matrix\n",
    "  #This looks at the shape of the parameters, and the delay, in order to create\n",
    "  #a delay matrix with numbers starting from the diagonal (the diagonal takes the value of the delay and the next values, \n",
    "  #follow an arithmetic progression with a unit increase)\n",
    "  size= max(rows, columns)\n",
    "  out = np.zeros((size, size))\n",
    "  for i in range(size):\n",
    "    del_new= np.ones(size-i)*(delay+i)\n",
    "    out = out + np.diag(del_new, i)\n",
    "  for i in range(1, size):\n",
    "    del_new = np.ones(size-i)*(delay-i)\n",
    "  return out[:rows, :columns]\n",
    "#Function lognormal_tensor added to the function delayed\n",
    "\n",
    "def itplt(arr, delay, datadelay): #Data smoothing function\n",
    "  itplt_data = tt.maximum(1 - tt.abs_(datadelay - delay), 0)\n",
    "  dotprod = tt.dot(arr, itplt_data)\n",
    "  return dotprod\n",
    "\n",
    "def delay_cases_lognormal(\n",
    "    input_arr,\n",
    "    len_input_arr,\n",
    "    len_output_arr,\n",
    "    median_delay,\n",
    "    scale_delay,\n",
    "    delay_t,\n",
    "):\n",
    "    delay_mat = delay(\n",
    "        rows=len_input_arr,\n",
    "        columns=len_output_arr,\n",
    "        delay=delay_t,\n",
    "    )\n",
    "    delay_mat[\n",
    "        delay_mat < 0.01\n",
    "    ] = 0.01  # needed because negative values lead to nans in the lognormal distribution.\n",
    "    delayed_arr = delayed(input_arr, median_delay, scale_delay, delay_mat)\n",
    "    return delayed_arr\n",
    "\n",
    "def delayed(arr, delay, delay_shape, datadelay): #apply_delay and tt_lognormal\n",
    "  distribution = tt.exp(-((tt.log(datadelay)-np.log(delay))**2)/ (2*delay_shape **2))\n",
    "  arr2 = distribution/tt.sum(distribution, axis=0)\n",
    "  return tt.dot(arr, arr2)\n",
    "\n",
    "def infer_delayed(I_rate_t,I_tperiod,  output_length, delay, diff_I_output):\n",
    "  delayed_initial = delay(rows= I_tperiod, columns = output_length, delay = diff_I_output)\n",
    "  delay_inferred = itplt(I_rate_t, delay, delayed_initial)\n",
    "  return delay_inferred\n",
    "\n",
    "def dist_smooth(v1, vk, t1, tk, t_total):\n",
    "  t = np.arange(t_total)\n",
    "  smooth = tt.clip((t - t1)/(tk- t1), 0,1) * (vk - v1) + v1 #Smoothing with delta values\n",
    "  return smooth \n",
    "#output=delay(rows= 6,columns = 4, delay = 2)\n",
    "#print(output)\n",
    "def SIR_MOD(daily_positive_cases, ordered_list_of_gov_interventions, date1, constant_parameters ):\n",
    "\n",
    "  \n",
    "  #Adding default values for when shapes and parameters are not defined in the change point list\n",
    "  #for key, value in prior_information_0.items():\n",
    "  #  if key not in prior_information:\n",
    "  #    prior_information[key] = value\n",
    "  \n",
    "\n",
    "  svi_mnrty_wt_avg = 0.8903 #CA:0.8903 #NY: 0.479 #NJ:0.66254\n",
    "  with pm.Model() as sim: \n",
    "    I_start = pm.HalfNormal(name = 'start_inf', sigma = constant_parameters['prior_beta_I_start']/(1+svi_mnrty_wt_avg))\n",
    "\n",
    "\n",
    "    list_infections = []\n",
    "\n",
    "    for i, sd_pt in enumerate(ordered_list_of_gov_interventions):\n",
    "      list_infections.append(\n",
    "          pm.Lognormal(\n",
    "              name=f'Inf_rate_{i}',\n",
    "              mu=np.log(sd_pt['prior_inf_rate_median'] ),\n",
    "              sigma = sd_pt['prior_inf_rate_sigma']\n",
    "          )\n",
    "      )\n",
    "          \n",
    "    cp_transient_list = []\n",
    "    prev_date = date1\n",
    "    for i, sd_pt in enumerate(ordered_list_of_gov_interventions[1:]):\n",
    "        \n",
    "        transient_start = sd_pt['prior_mean_transient']\n",
    "        prior_mean = (transient_start - prev_date).days\n",
    "      \n",
    "        tr_start = pm.Normal(\n",
    "                name=f\"transient_start_{i}\",\n",
    "                mu=prior_mean,\n",
    "                sigma=sd_pt[\"prior_variance_date_start_transient\"],\n",
    "            )\n",
    "        cp_transient_list.append(tr_start)\n",
    "        dt_before = transient_start\n",
    "        # same for transient times\n",
    "    tr_len_list = []\n",
    "    for i, cp in enumerate(ordered_list_of_gov_interventions[1:]):\n",
    "      tr_len = pm.Lognormal(\n",
    "          name=f\"transient_len_{i}\",\n",
    "          mu=np.log(cp[\"prior_median_transient_len\"]),\n",
    "          sigma=cp[\"prior_variance_transient_len\"],)\n",
    "      tr_len_list.append(tr_len)\n",
    "\n",
    "    Inf_rate_t_list = [list_infections[0] * tt.ones(constant_parameters['num_days_simulation'])]\n",
    "    Inf_rate_before = list_infections[0]\n",
    "\n",
    "    for tr_start, tr_len, Inf_rate_after in zip(\n",
    "            cp_transient_list, tr_len_list, list_infections[1:]\n",
    "        ):\n",
    "        Inf_rate_t = dist_smooth(\n",
    "                          v1=0,\n",
    "                vk=1,\n",
    "                t1=tr_start,\n",
    "                tk=tr_start + tr_len,\n",
    "                t_total=constant_parameters['num_days_simulation'],\n",
    "            ) * (Inf_rate_after - Inf_rate_before)\n",
    "        Inf_rate_before = Inf_rate_after\n",
    "        Inf_rate_t_list.append(Inf_rate_t)\n",
    "    Inf_rate_t = sum(Inf_rate_t_list)\n",
    "\n",
    "        # fraction of people that recover each day, recovery rate mu\n",
    "    mu = pm.Lognormal(\n",
    "            name=\"mu\",\n",
    "            mu=np.log(constant_parameters[\"prior_median_mu\"]),\n",
    "            sigma=constant_parameters[\"prior_variance_mu\"],\n",
    "        )\n",
    "\n",
    "        # delay in days between contracting the disease and being recorded\n",
    "    delay = pm.Lognormal(\n",
    "            name=\"delay\",\n",
    "            mu=np.log(constant_parameters[\"prior_median_delay\"]),\n",
    "            sigma=constant_parameters[\"prior_variance_delay\"],\n",
    "        )\n",
    "\n",
    "        # prior of the error of observed cases\n",
    "    sigma_obs = pm.HalfNormal(\"sigma_obs\", sigma=constant_parameters[\"prior_beta_variance_obs\"])\n",
    "\n",
    "        # -------------------------------------------------------------------------- #\n",
    "        # training the model with loaded data provided as argument\n",
    "        # -------------------------------------------------------------------------- #\n",
    "\n",
    "    S_start = constant_parameters['tot_pop'] - I_start\n",
    "\n",
    "    new_I_0 = tt.zeros_like(I_start)\n",
    "    S, I, new_I = Model_simulation(\n",
    "            Inf_rate_t=Inf_rate_t, mu=mu, S_start=S_start, I_start=I_start, N=constant_parameters['tot_pop']\n",
    "        )\n",
    "    '''\n",
    "    def subsequent_day_parameters(Inf_rate_t, S_t, I_t, _,mu, tot_pop):\n",
    "      new_I_t = Inf_rate_t / tot_pop * I_t * S_t\n",
    "      S_t = S_t - new_I_t\n",
    "      I_t = I_t + new_I_t - mu * I_t\n",
    "      I_t = tt.clip(I_t, 0, tot_pop) #for stability\n",
    "      return S_t, I_t, new_I_t\n",
    "\n",
    "      # first tuple of theano scan will return S, I, new_I\n",
    "    outputs, _ = theano.scan(\n",
    "        fn=subsequent_day_parameters,\n",
    "        sequences=[Inf_rate_t],\n",
    "        outputs_info=[S_start, I_start, new_I_0],\n",
    "        non_sequences=[mu, constant_parameters['tot_pop']],)\n",
    "\n",
    "    S, I, new_I = outputs\n",
    "    '''\n",
    "\n",
    "    new_cases_inferred = delay_cases(\n",
    "        new_I_t=new_I,\n",
    "        len_new_I_t=constant_parameters['num_days_simulation'],\n",
    "        len_out=constant_parameters['num_days_simulation'] - constant_parameters['diff_data_simulation'],\n",
    "        delay=delay,\n",
    "        delay_diff=constant_parameters['diff_data_simulation'],)\n",
    "    \n",
    "    \n",
    "    new_cases_inferred_eff = new_cases_inferred\n",
    "    num_days_data = daily_positive_cases.shape[-1]\n",
    "    \n",
    "    pm.StudentT(\n",
    "        name=\"_new_cases_studentT\",\n",
    "        nu=4,\n",
    "        mu=new_cases_inferred_eff[:num_days_data],\n",
    "        sigma=tt.abs_(new_cases_inferred[:num_days_data] + 1) ** 0.5\n",
    "        * sigma_obs,  #+1 and tt.abs to avoid nans\n",
    "        observed=daily_positive_cases,)\n",
    "    \n",
    "    pm.Deterministic(\"Inf_rate_t\", Inf_rate_t)\n",
    "    pm.Deterministic(\"new_cases\", new_cases_inferred_eff)\n",
    "    pm.Deterministic(\"new_cases_raw\", new_cases_inferred)\n",
    "\n",
    "  return sim\n",
    "\n",
    "def Model_simulation(Inf_rate_t, mu, S_start, I_start, N):\n",
    "    \"\"\"\n",
    "        Implements the susceptible-infected-recovered model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Inf_rate_t : ~numpy.ndarray\n",
    "            time series of spreading rate, the length of the array sets the\n",
    "            number of steps to run the model for\n",
    "\n",
    "        mu : number\n",
    "            recovery rate\n",
    "\n",
    "        S_start : number\n",
    "            initial number of susceptible at first time step\n",
    "\n",
    "        I_start : number\n",
    "            initial number of infected\n",
    "\n",
    "        N : number\n",
    "            population size\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        S : array\n",
    "            time series of the susceptible\n",
    "\n",
    "        I : array\n",
    "            time series of the infected\n",
    "\n",
    "        new_I : array\n",
    "            time series of the new infected\n",
    "    \"\"\"\n",
    "\n",
    "    new_I_0 = tt.zeros_like(I_start)\n",
    "\n",
    "    def next_day(Inf_rate_t, S_t, I_t, _, mu, N):\n",
    "        new_I_t = Inf_rate_t / N * I_t * S_t\n",
    "        S_t = S_t - new_I_t\n",
    "        I_t = I_t + new_I_t - mu * I_t\n",
    "        I_t = tt.clip(I_t, 0, N)  # for stability\n",
    "        return S_t, I_t, new_I_t\n",
    "\n",
    "    # theano scan returns two tuples, first one containing a time series of\n",
    "    # what we give in outputs_info : S, I, new_I\n",
    "    outputs, _ = theano.scan(\n",
    "        fn=next_day,\n",
    "        sequences=[Inf_rate_t],\n",
    "        outputs_info=[S_start, I_start, new_I_0],\n",
    "        non_sequences=[mu, N],\n",
    "    )\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "NsTyL1tGy50h",
    "outputId": "7f4cfac3-19da-409d-883b-604da170d109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/amanchawla'"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "id": "gWwJCtitSDuy",
    "outputId": "90c62ad9-645d-48f4-c05f-0f33e43b6685"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Dates  AK  AL  AR  AZ  CA  CO  CT  DC  DE  FL  GA  HI  IA  ID  IL  IN  \\\n",
      "0  1/22/20   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "1  1/23/20   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "2  1/24/20   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   \n",
      "3  1/25/20   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   \n",
      "4  1/26/20   0   0   0   1   2   0   0   0   0   0   0   0   0   0   1   0   \n",
      "\n",
      "   KS  KY  LA  MA  MD  ME  MI  MN  MO  MS  MT  NC  ND  NE  NH  NJ  NM  NV  NY  \\\n",
      "0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "3   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "4   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "\n",
      "   OH  OK  OR  PA  RI  SC  SD  TN  TX  UT  VA  VT  WA  WI  WV  WY  \n",
      "0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0  \n",
      "1   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0  \n",
      "2   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0  \n",
      "3   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0  \n",
      "4   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using advi...\n",
      "Average Loss = 304.08:   7%|▋         | 13691/200000 [02:03<28:07, 110.43it/s]\n",
      "Convergence achieved at 13700\n",
      "Interrupted at 13,699 [6%]: Average Loss = 407.92\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [sigma_obs, delay, mu, Inf_rate_0, start_inf]\n",
      "Sampling 2 chains, 0 divergences: 100%|██████████| 10000/10000 [46:42<00:00,  3.57draws/s]\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using advi...\n",
      "Average Loss = 292.88:   8%|▊         | 15893/200000 [02:30<29:04, 105.52it/s]\n",
      "Convergence achieved at 15900\n",
      "Interrupted at 15,899 [7%]: Average Loss = 385.81\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [sigma_obs, delay, mu, transient_len_0, transient_start_0, Inf_rate_1, Inf_rate_0, start_inf]\n",
      "Sampling 2 chains, 0 divergences: 100%|██████████| 10000/10000 [2:18:45<00:00,  1.20draws/s]\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using advi...\n",
      "Average Loss = 292.89:   8%|▊         | 16489/200000 [02:36<29:06, 105.09it/s]\n",
      "Convergence achieved at 16500\n",
      "Interrupted at 16,499 [8%]: Average Loss = 382.77\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [sigma_obs, delay, mu, transient_len_1, transient_len_0, transient_start_1, transient_start_0, Inf_rate_2, Inf_rate_1, Inf_rate_0, start_inf]\n",
      "Sampling 2 chains, 0 divergences: 100%|██████████| 10000/10000 [2:55:19<00:00,  1.05s/draws]\n",
      "The number of effective samples is smaller than 25% for some parameters.\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('Confirmed_cases_by_state.csv') #FOR STATE DATA\n",
    "#data=pd.read_csv('C:\\\\Users\\\\Aman\\\\Documents\\\\covid_confirmed_usafacts_1604.csv') #FOR COUNTY DATA\n",
    "\n",
    "population_lockdown_dates=pd.read_csv('LockdownDates_populationMerged.csv')\n",
    "country = 'United States'\n",
    "state_codes=data.columns[1:]\n",
    "\n",
    "date_data_begin = max(pd.to_datetime(data['Dates'], infer_datetime_format=True)).to_pydatetime() - datetime.timedelta(days=40)\n",
    "date_data_end   = max(pd.to_datetime(data['Dates'], infer_datetime_format=True)).to_pydatetime()\n",
    "data2=data[pd.to_datetime(data['Dates'], infer_datetime_format=True)>=date_data_begin]\n",
    "\n",
    "\n",
    "population_lockdown_dates.loc[population_lockdown_dates['lockdown_date']=='YTA','lockdown_date']=date_data_end\n",
    "population_lockdown_dates.loc[population_lockdown_dates['partial_date']=='YTA','lockdown_date']=date_data_end\n",
    "population_lockdown_dates.loc[population_lockdown_dates['partial_date']=='YTA','partial_date']=date_data_end\n",
    "\n",
    "population_lockdown_dates['partial_date']=population_lockdown_dates['partial_date'].fillna(date_data_end)\n",
    "population_lockdown_dates['lockdown_date']=population_lockdown_dates['lockdown_date'].fillna(date_data_end)\n",
    "population_lockdown_dates['lockdown_date']=pd.to_datetime(population_lockdown_dates['lockdown_date'], infer_datetime_format=True)\n",
    "population_lockdown_dates.loc[population_lockdown_dates['lockdown_date']<date_data_begin,'lockdown_date']=date_data_end\n",
    "population_lockdown_dates['partial_date']=pd.to_datetime(population_lockdown_dates['partial_date'], infer_datetime_format=True)\n",
    "population_lockdown_dates.loc[population_lockdown_dates['lockdown_date']<population_lockdown_dates['partial_date'],'partial_date']=population_lockdown_dates['lockdown_date']\n",
    "population_lockdown_dates.dtypes\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(data.head())\n",
    "\n",
    "\n",
    "#FOR STATE DATA (i should correspond to the state code we're running it for in state_code)\n",
    "i=4 #4: California; 31: NEw Jersey, 34: New York\n",
    "state=state_codes[i]\n",
    "cases_obs = data2[state].values\n",
    "\n",
    "population_lockdown_dates_sub=population_lockdown_dates.loc[population_lockdown_dates['State Code']==state]\n",
    "population_lockdown_dates_sub\n",
    "\n",
    "#prior_date_mild_dist_begin = datetime.datetime.utcfromtimestamp(population_lockdown_dates_sub['partial_date'].values[0].tolist()/1e9)- datetime.timedelta(days = 3) \n",
    "#prior_date_mild_dist_begin\n",
    "\n",
    "diff_data_sim = 14\n",
    "date_begin_sim = date_data_begin - datetime.timedelta(days = diff_data_sim) \n",
    "\n",
    "prior_date_mild_dist_begin = datetime.datetime.utcfromtimestamp(population_lockdown_dates_sub['partial_date'].values[0].tolist()/1e9)- datetime.timedelta(days = 3) \n",
    "prior_date_strong_dist_begin =  datetime.datetime.utcfromtimestamp(population_lockdown_dates_sub['partial_date'].values[0].tolist()/1e9)- datetime.timedelta(days = 2) \n",
    "prior_date_contact_ban_begin =  datetime.datetime.utcfromtimestamp(population_lockdown_dates_sub['lockdown_date'].values[0].tolist()/1e9)- datetime.timedelta(days = 1) \n",
    "\n",
    "num_days_data = (date_data_end-date_data_begin).days\n",
    "num_days_future = 110\n",
    "date_begin_sim = date_data_begin - datetime.timedelta(days = diff_data_sim)\n",
    "date_end_sim   = date_data_end   + datetime.timedelta(days = num_days_future)\n",
    "num_days_sim = (date_end_sim-date_begin_sim).days\n",
    "\n",
    "\n",
    "cases_obs = data2[state].values\n",
    "\n",
    "#what if scenarios\n",
    "#intervention attribute dictionary\n",
    "\n",
    "\n",
    "###################################################################\n",
    "#dictionary format for adding new goverment interventions##########\n",
    "###################################################################\n",
    "\n",
    "\n",
    "intervetion_attributes = dict(\n",
    "    #date of intervention begin\n",
    "    prior_mean_transient=None,\n",
    "    prior_variance_date_start_transient=None,\n",
    "    \n",
    "    #time for intervebtion to take effect\n",
    "    prior_median_transient_len=None,\n",
    "    prior_variance_transient_len=None,\n",
    "    \n",
    "    #infection rate\n",
    "    prior_inf_rate_median= None,\n",
    "    prior_inf_rate_sigma= None,\n",
    "    \n",
    ")\n",
    "\n",
    "###################################################################\n",
    "#define new goverment interventions################################\n",
    "###################################################################\n",
    "\n",
    "#defining default prior\n",
    "default_prior = {\n",
    "    #date of intervention begin\n",
    "    'prior_mean_transient':None,\n",
    "    'prior_variance_date_start_transient':None,                \n",
    "    #time for intervebtion to take effect\n",
    "    'prior_median_transient_len':None,\n",
    "    'prior_variance_transient_len':None,\n",
    "    #infection rate\n",
    "    'prior_inf_rate_median': 0.4,\n",
    "    'prior_inf_rate_sigma': 0.9}\n",
    "                    \n",
    "#defining changept 2 -  mild_social_distancing\n",
    "mild_social_distancing = {\n",
    "                    #date of intervention begin\n",
    "                    'prior_mean_transient':prior_date_mild_dist_begin,\n",
    "                    'prior_variance_date_start_transient':3,\n",
    "                    \n",
    "                    #time for intervebtion to take effect\n",
    "                    'prior_median_transient_len':3,\n",
    "                    'prior_variance_transient_len':0.3,\n",
    "                    \n",
    "                    #infection rate\n",
    "                    'prior_inf_rate_median': 0.2,\n",
    "                    'prior_inf_rate_sigma': 0.5\n",
    "                    \n",
    "                }\n",
    "\n",
    "#defining changept 3 -  strong social distancing\n",
    "strong_social_distancing = {\n",
    "                    #date of intervention begin\n",
    "                    'prior_mean_transient':prior_date_strong_dist_begin,\n",
    "                    'prior_variance_date_start_transient':1,\n",
    "                    \n",
    "                    #time for intervebtion to take effect\n",
    "                    'prior_median_transient_len':3,\n",
    "                    'prior_variance_transient_len':0.3,\n",
    "                    \n",
    "                    #infection rate\n",
    "                    'prior_inf_rate_median': 1/8,\n",
    "                    'prior_inf_rate_sigma': 0.5\n",
    "                    \n",
    "                }\n",
    "\n",
    "\n",
    "#defining changept 3 -  strong social distancing\n",
    "lock_down = {\n",
    "                    #date of intervention begin\n",
    "                    'prior_mean_transient':prior_date_contact_ban_begin,\n",
    "                    'prior_variance_date_start_transient':1,\n",
    "                    \n",
    "                    #time for intervebtion to take effect\n",
    "                    'prior_median_transient_len':3,\n",
    "                    'prior_variance_transient_len':0.3,\n",
    "                    \n",
    "                    #infection rate\n",
    "                    'prior_inf_rate_median': 1/8/2,\n",
    "                    'prior_inf_rate_sigma': 0.5\n",
    "                    \n",
    "                }\n",
    "\n",
    "\n",
    "ordered_list_of_gov_interventions = [ default_prior, mild_social_distancing, strong_social_distancing, lock_down]\n",
    "\n",
    "\n",
    "constant_parameters = {\n",
    "          'tot_pop': population_lockdown_dates_sub['Population'].values[0],\n",
    "          'prior_beta_I_start' : 100,\n",
    "          'prior_median_mu' : 0.12, \n",
    "          'prior_variance_mu' : 0.2,\n",
    "          'prior_median_delay' : 5, \n",
    "          'prior_variance_delay' : 0.2,\n",
    "          'prior_beta_variance_obs' : 10,\n",
    "          #simulation inofrmation\n",
    "          'num_days_simulation' : num_days_sim,\n",
    "          'diff_data_simulation' : diff_data_sim\n",
    "          }\n",
    "\n",
    "traces = []\n",
    "models=[]\n",
    "for scenarios in np.arange(1,4):\n",
    "  model = SIR_MOD(daily_positive_cases= np.diff(cases_obs), \n",
    "                ordered_list_of_gov_interventions = ordered_list_of_gov_interventions[:scenarios], \n",
    "                date1 = date_begin_sim, \n",
    "                constant_parameters = constant_parameters)\n",
    "  models.append(model)\n",
    "  traces.append(pm.sample(model=model, init='advi', draws=4000, tune=1000 ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kJC3DH9omreF"
   },
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 686
    },
    "colab_type": "code",
    "id": "qeZHjYgIJZvi",
    "outputId": "27f72bdd-f71d-4895-e819-e6482bdd017a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Strong Social Distancing'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-6ccf928af77a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcases_future\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         np.percentile(cases_future, q=97.5, axis=-1),)\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mcum_cases_Future_percentiles_DF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'{point}- Lower Bound'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCUM_Cases_Future_percentiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0mcum_cases_Future_percentiles_DF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'{point} - Upper Bound'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCUM_Cases_Future_percentiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2978\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2979\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2980\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2982\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Strong Social Distancing'"
     ]
    }
   ],
   "source": [
    "def truncate_number(number, precision):\n",
    "    return '{{:.{}f}}'.format(precision).format(number)  \n",
    "\n",
    "def print_median_CI(arr, prec = 2):\n",
    "    f_trunc = lambda n: truncate_number(n, prec)\n",
    "    med = f_trunc(np.median(arr))\n",
    "    perc1, perc2 = f_trunc(np.percentile(arr, q=2.5)), f_trunc(np.percentile(arr, q=97.5))\n",
    "    return 'Median: {}\\n95% CI: [{}, {}]'.format(med, perc1, perc2)\n",
    "def conv_time_to_mpl_dates(arr):\n",
    "    return matplotlib.dates.date2num([datetime.timedelta(days=float(date)) + date_begin_sim for date in arr])\n",
    "traces_copy=traces\n",
    "trace = traces[0]\n",
    "posterior = traces\n",
    "\n",
    "colors  = ['tab:green']\n",
    "points = [ 'Mild Social Distancing', 'Strong Social Distancing','Total Lockdown']\n",
    "\n",
    "new_cases_past=pd.DataFrame()\n",
    "new_cases_Future=pd.DataFrame()\n",
    "new_cases_Future_percentiles=pd.DataFrame()\n",
    "new_cases_Future_percentiles_DF=pd.DataFrame()\n",
    "for trace_scen, color, point in zip(posterior, colors, points):\n",
    "    new_cases_past1 = trace_scen.new_cases[:,:num_days_data]\n",
    "    new_cases_past[point]=np.median(new_cases_past1, axis=0)\n",
    "    time2 = np.arange(0, num_days_future+1)\n",
    "    mpl_dates_fut = conv_time_to_mpl_dates(time2) + diff_data_sim + num_days_data\n",
    "    end_date = mpl_dates_fut[-10]\n",
    "    cases_future = trace_scen['new_cases'][:, num_days_data:].T\n",
    "    new_cases_Future[point] = np.median(cases_future, axis=-1)\n",
    "    new_cases_Future_percentiles[point] = (\n",
    "    np.percentile(cases_future, q=2.5, axis=-1),\n",
    "    np.percentile(cases_future, q=97.5, axis=-1),\n",
    "    )\n",
    "   \n",
    "    new_cases_Future_percentiles_DF[f'{point} - Lower Bound'] = new_cases_Future_percentiles[point][0]\n",
    "    new_cases_Future_percentiles_DF[f'{point} - Upper Bound'] = new_cases_Future_percentiles[point][1]\n",
    "    \n",
    "Date_sim=[]\n",
    "for i in range(1, len(time2)):\n",
    "  Date_sim.append(date_begin_sim +datetime.timedelta(days=float(time2[i]))+datetime.timedelta(days=float(54)))\n",
    "date_sim2=[]\n",
    "for date in Date_sim:\n",
    "  date_sim2.append(datetime.datetime.strftime(date, '%d/%m/%y'))\n",
    "print(len(date_sim2[1:]))\n",
    "date_sim3=date_sim2[1:]\n",
    "new_cases_Future\n",
    "new_cases_Future_cum=new_cases_Future.cumsum(axis=0)\n",
    "new_cases_Future_cum1=new_cases_Future_cum+cases_obs[-1]\n",
    "new_cases_Future_cum1\n",
    "\n",
    "new_cases_Future_cum2=new_cases_Future_cum1\n",
    "new_cases_Future_cum2.index=date_sim2\n",
    "new_cases_Future_cum2 #SAVE THIS \n",
    "CUM_Cases_Future_percentiles=pd.DataFrame()\n",
    "Cumulative_Cases=pd.DataFrame()\n",
    "cum_cases_Future_percentiles_DF=pd.DataFrame()\n",
    "for trace_scen,  point in zip(posterior, points[1:]):\n",
    "    new_cases_past = trace_scen.new_cases[:,:num_days_data]\n",
    "    cum_cases = np.cumsum(new_cases_past, axis=1) + cases_obs[0]\n",
    "    Cumulative_Cases[point]=np.median(cum_cases, axis=0)\n",
    "    time2 = np.arange(0, num_days_future+1)\n",
    "    mpl_dates_fut = conv_time_to_mpl_dates(time2) + diff_data_sim + num_days_data\n",
    "    cases_future = np.cumsum(trace_scen['new_cases'][:, num_days_data:].T, axis=0) + cases_obs[-1]\n",
    "    \n",
    "    #cases_future = np.concatenate([np.ones((1,cases_future.shape[1]))*cases_obs[-1], cases_future], axis=0)\n",
    "    #Cumulative_Cases[legend] = np.median(cases_future, axis=-1)\n",
    "    CUM_Cases_Future_percentiles[legend] = (\n",
    "        np.percentile(cases_future, q=2.5, axis=-1),\n",
    "        np.percentile(cases_future, q=97.5, axis=-1),)\n",
    "    cum_cases_Future_percentiles_DF[f'{point}- Lower Bound'] = CUM_Cases_Future_percentiles[point][0]\n",
    "    cum_cases_Future_percentiles_DF[f'{point} - Upper Bound'] = CUM_Cases_Future_percentiles[point][1]\n",
    "\n",
    "\n",
    "cum_cases_Future_percentiles_DF.index=date_sim2\n",
    "cum_cases_Future_percentiles_DF\n",
    "cum_cases_Future_percentiles_DF['Date']=cum_cases_Future_percentiles_DF.index.astype('str')\n",
    "new_cases_Future_cum2['Date']=new_cases_Future_cum2.index.astype('str')\n",
    "FINAL_DF=pd.merge(cum_cases_Future_percentiles_DF, new_cases_Future_cum2, on=\"Date\")\n",
    "\n",
    "\n",
    "FINAL_DF.index=FINAL_DF['Date']\n",
    "FINAL_DF2=FINAL_DF.copy()\n",
    "\n",
    "#FINAL_DF2.to_csv('California_0804_Projection_Data2_1.csv')\n",
    "#FINAL_DF2.to_csv('California_2604_Projection_Data2_1.csv')\n",
    "#FINAL_DF2.to_csv('N_1604_Projection_Data2_1.csv')\n",
    "FINAL_DF2.head()\n",
    "FINAL_DF2.to_csv('CA_new_code_projection_08_04_3scenarios.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "id": "IKt1KjhrdVS0",
    "outputId": "79dd5d04-5419-4ad6-a411-9c41044fe2fd"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-9f720bb91a5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mposterior\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'new_cases'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_days_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8PENIYzZR5nP"
   },
   "outputs": [],
   "source": [
    "def delay(rows, columns, delay=0): #make_Delay_matrix\n",
    "  #This looks at the shape of the parameters, and the delay, in order to create\n",
    "  #a delay matrix with numbers starting from the diagonal (the diagonal takes the value of the delay and the next values, \n",
    "  #follow an arithmetic progression with a unit increase)\n",
    "  size= max(rows, columns)\n",
    "  out = np.zeros((size, size))\n",
    "  for i in range(size):\n",
    "    del_new= np.ones(size-i)*(delay+i)\n",
    "    out = out + np.diag(del_new, i)\n",
    "  for i in range(1, size):\n",
    "    del_new = np.ones(size-i)*(delay-i)\n",
    "  return out[:rows, :columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "colab_type": "code",
    "id": "sHAgWPFvR6_9",
    "outputId": "d3496f50-9c0b-4a97-bde4-7a8b93f45192"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14., 15., 16., ..., 61., 62., 63.],\n",
       "       [ 0., 14., 15., ..., 60., 61., 62.],\n",
       "       [ 0.,  0., 14., ..., 59., 60., 61.],\n",
       "       ...,\n",
       "       [ 0.,  0.,  0., ..., 14., 15., 16.],\n",
       "       [ 0.,  0.,  0., ...,  0., 14., 15.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0., 14.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delay(50,50,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "q1VLz3_aTHLL",
    "outputId": "a078249e-878e-4ff0-f5d5-9f66eafe78b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(1, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "6Z64t6ZlTXv_",
    "outputId": "f4a4751d-4c29-4013-da21-91348ffc95e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(1, 4)\n"
     ]
    }
   ],
   "source": [
    "print(range(1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "pzDHerQgUAvh",
    "outputId": "865d348b-2d53-4314-818d-7bb415451445"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "4U-3tPZ-UHXK",
    "outputId": "8153a28b-7668-4a13-c636-96a0a3cc1655"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(1, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5QuIQHKkGNjh"
   },
   "source": [
    "Authors: Sachin C S (Cognizant), Aman Chawla (Cognizant)\n",
    "\n",
    "Copyright {2020} Cognizant Technology Solutions\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NEW_CODE_RUN_v0_3_SVI_mnrty_All_Scenarios.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
